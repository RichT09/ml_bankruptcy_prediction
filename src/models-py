# 03_model_training.py

"""
Module 03: Model Training
Base models + Tuned models with hyperparameter optimization
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
import pickle
import logging
from pathlib import Path
import sys
from typing import Dict, Tuple
import time

# Setup logging
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Add project root
PROJECT_ROOT = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(PROJECT_ROOT))

try:
    from src.config import config
except ImportError:
    import config


# ============================================================================
# MODEL BUILDING - BASE VERSION
# ============================================================================

def build_base_models(class_weights, scale_pos_weight, verbose: bool = True) -> Dict:
    """
    Build base models with default/conservative hyperparameters
    
    Args:
        class_weights: Dictionary of class weights
        scale_pos_weight: XGBoost weight parameter
        verbose: Print model info
    
    Returns:
        Dictionary of base models
    """
    if verbose:
        logger.info("\n--- Building BASE Models ---")
    
    models_base = {}
    
    # Logistic Regression
    logreg = LogisticRegression(**config.models.logreg_base)
    models_base['LogisticRegression_base'] = logreg
    if verbose:
        logger.info("‚úì LogisticRegression (base)")
    
    # Random Forest
    rf = RandomForestClassifier(**config.models.rf_base)
    models_base['RandomForest_base'] = rf
    if verbose:
        logger.info("‚úì RandomForest (base)")
    
    # XGBoost
    xgb_params = config.models.xgb_base.copy()
    xgb_params['scale_pos_weight'] = scale_pos_weight
    xgb = XGBClassifier(**xgb_params)
    models_base['XGBoost_base'] = xgb
    if verbose:
        logger.info("‚úì XGBoost (base)")
    
    # # SVM
    # svm = SVC(**config.models.svm_base)
    # models_base['SVM_base'] = svm
    # if verbose:
    #     logger.info("‚úì SVM (base)")
    
    return models_base


# ============================================================================
# MODEL BUILDING - TUNED VERSION
# ============================================================================

def build_tuned_models(class_weights, scale_pos_weight, verbose: bool = True) -> Dict:
    """
    Build tuned models with optimized hyperparameters
    
    Args:
        class_weights: Dictionary of class weights
        scale_pos_weight: XGBoost weight parameter
        verbose: Print model info
    
    Returns:
        Dictionary of tuned models
    """
    if verbose:
        logger.info("\n--- Building TUNED Models ---")
    
    models_tuned = {}
    
    # Logistic Regression
    logreg = LogisticRegression(**config.models.logreg_tuned)
    models_tuned['LogisticRegression_tuned'] = logreg
    if verbose:
        logger.info("‚úì LogisticRegression (tuned)")
    
    # Random Forest
    rf = RandomForestClassifier(**config.models.rf_tuned)
    models_tuned['RandomForest_tuned'] = rf
    if verbose:
        logger.info("‚úì RandomForest (tuned)")
    
    # XGBoost
    xgb_params = config.models.xgb_tuned.copy()
    xgb_params['scale_pos_weight'] = scale_pos_weight
    xgb = XGBClassifier(**xgb_params)
    models_tuned['XGBoost_tuned'] = xgb
    if verbose:
        logger.info("‚úì XGBoost (tuned)")
    
    # # SVM
    # svm = SVC(**config.models.svm_tuned)
    # models_tuned['SVM_tuned'] = svm
    # if verbose:
    #     logger.info("‚úì SVM (tuned)")
    
    return models_tuned


# ============================================================================
# MODEL TRAINING
# ============================================================================

def fit_models(
    models: Dict,
    X_train,
    X_train_scaled,
    y_train,
    X_train_smote=None,
    X_train_smote_scaled=None,
    y_train_smote=None,
    verbose: bool = True
) -> Dict:
    """
    Train all models
    
    Linear models (LogReg, SVM) use scaled data
    Tree models (RF, XGBoost) use raw data
    
    Args:
        models: Dictionary of models to train
        X_train: Raw training features
        X_train_scaled: Scaled training features
        y_train: Training target
        X_train_smote: SMOTE resampled features (optional)
        X_train_smote_scaled: SMOTE scaled features (optional)
        y_train_smote: SMOTE target (optional)
        verbose: Print progress
    
    Returns:
        Dictionary of fitted models
    """
    use_smote = X_train_smote is not None and config.models.use_smote
    
    if verbose:
        logger.info(f"\n--- Training Models (SMOTE: {use_smote}) ---")
    
    for name, model in models.items():
        start_time = time.time()
        
        # Determine which data to use
        if 'LogisticRegression' in name or 'SVM' in name:
            X = X_train_smote_scaled if use_smote else X_train_scaled
            y = y_train_smote if use_smote else y_train
        else:  # Tree-based models
            X = X_train_smote if use_smote else X_train
            y = y_train_smote if use_smote else y_train
        
        # Fit model
        model.fit(X, y)
        
        elapsed = time.time() - start_time
        if verbose:
            logger.info(f"‚úì {name:30s} trained in {elapsed:6.2f}s  |  {X.shape[0]:,} samples")
    
    return models


# ============================================================================
# MODEL SAVING
# ============================================================================

def save_models(models_base, models_tuned, scaler, metadata: Dict, verbose: bool = True):
    """
    Save all models and metadata
    
    Args:
        models_base: Dictionary of base models
        models_tuned: Dictionary of tuned models
        scaler: Fitted scaler
        metadata: Additional information
        verbose: Print save paths
    """
    # Save base models
    base_path = config.models_dir / "models_base.pkl"
    with open(base_path, 'wb') as f:
        pickle.dump(models_base, f)
    if verbose:
        logger.info(f"\n‚úì Saved base models: {base_path}")
    
    # Save tuned models
    tuned_path = config.models_dir / "models_tuned.pkl"
    with open(tuned_path, 'wb') as f:
        pickle.dump(models_tuned, f)
    if verbose:
        logger.info(f"‚úì Saved tuned models: {tuned_path}")
    
    # Save scaler
    scaler_path = config.models_dir / "scaler.pkl"
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    if verbose:
        logger.info(f"‚úì Saved scaler: {scaler_path}")
    
    # Save metadata
    meta_path = config.models_dir / "metadata.pkl"
    with open(meta_path, 'wb') as f:
        pickle.dump(metadata, f)
    if verbose:
        logger.info(f"‚úì Saved metadata: {meta_path}")


# ============================================================================
# MAIN PIPELINE
# ============================================================================

def run_models_pipeline(data: Dict, verbose: bool = True) -> Dict:
    """
    Complete model training pipeline
    
    Args:
        data: Preprocessed data dictionary from module 02
        verbose: Print progress
    
    Returns:
        Dictionary with base and tuned models
    """
    logger.info("\n" + "="*70)
    logger.info("MODULE 03: MODEL TRAINING (BASE + TUNED)")
    logger.info("="*70)
    
    # Extract data
    X_train = data['X_train']
    X_train_scaled = data['X_train_scaled']
    y_train = data['y_train']
    X_train_smote = data.get('X_train_smote')
    X_train_smote_scaled = data.get('X_train_smote_scaled')
    y_train_smote = data.get('y_train_smote')
    class_weights = data['class_weights']
    scale_pos_weight = data['scale_pos_weight']
    scaler = data['scaler']
    
    # 1. Build base models
    models_base = build_base_models(class_weights, scale_pos_weight, verbose=verbose)
    
    # 2. Build tuned models
    models_tuned = build_tuned_models(class_weights, scale_pos_weight, verbose=verbose)
    
    # 3. Train base models
    logger.info("\n" + "-"*70)
    logger.info("Training BASE Models")
    logger.info("-"*70)
    models_base = fit_models(
        models_base,
        X_train, X_train_scaled, y_train,
        X_train_smote, X_train_smote_scaled, y_train_smote,
        verbose=verbose
    )
    
    # 4. Train tuned models
    logger.info("\n" + "-"*70)
    logger.info("Training TUNED Models")
    logger.info("-"*70)
    models_tuned = fit_models(
        models_tuned,
        X_train, X_train_scaled, y_train,
        X_train_smote, X_train_smote_scaled, y_train_smote,
        verbose=verbose
    )
    
    # 5. Save models
    metadata = {
        'train_size': len(X_train),
        'n_features': X_train.shape[1],
        'class_weights': class_weights,
        'scale_pos_weight': float(scale_pos_weight),
        'use_smote': config.models.use_smote,
        'split_year': config.data.split_year,
    }
    
    save_models(models_base, models_tuned, scaler, metadata, verbose=verbose)
    
    logger.info("\n" + "="*70)
    logger.info("‚úÖ MODULE 03 COMPLETED")
    logger.info(f"  Base models: {len(models_base)}")
    logger.info(f"  Tuned models: {len(models_tuned)}")
    logger.info("="*70)
    
    return {
        'models_base': models_base,
        'models_tuned': models_tuned,
    }


if __name__ == "__main__":
    # Load preprocessed data
    import pickle
    
    data_path = config.models_dir / "preprocessed_data.pkl"
    if not data_path.exists():
        logger.error(f"‚ùå {data_path} not found. Run module 02 first.")
        sys.exit(1)
    
    with open(data_path, 'rb') as f:
        data = pickle.load(f)
    
    # Run training
    models = run_models_pipeline(data, verbose=True)
    
    print(f"\nüéØ Training complete!")
    print(f"  Base models: {list(models['models_base'].keys())}")
    print(f"  Tuned models: {list(models['models_tuned'].keys())}")
